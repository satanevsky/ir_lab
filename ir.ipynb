{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article = namedtuple('Article', ['id', 'title', 'authors', 'info', 'abstract'])\n",
    "Query = namedtuple('Query', ['id', 'query_string'])\n",
    "Document = namedtuple('Document', ['id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_COUNT = 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = list()\n",
    "with open('data/cran.all.1400') as f:\n",
    "    raw_text = f.read()\n",
    "documents_data = raw_text.split('.I')[1:]\n",
    "assert len(documents_data) == DOCS_COUNT\n",
    "for (i, doc_data) in enumerate(documents_data):\n",
    "    (doc_id, doc_data) = doc_data.split('.T')\n",
    "    doc_id = int(doc_id.strip())\n",
    "    if doc_id != 240:\n",
    "        (title, doc_data) = doc_data.split('.A')\n",
    "    else:\n",
    "        (title, doc_data, _) = doc_data.split('.A')\n",
    "    title = title.strip()\n",
    "    (authors, doc_data) = doc_data.split('.B')\n",
    "    authors = authors.strip()\n",
    "    if doc_id not in (576, 578):\n",
    "        (info, abstract) = doc_data.split('.W')\n",
    "    else:\n",
    "        (info, abstract, _) = doc_data.split('.W')\n",
    "    info = info.strip()\n",
    "    info = abstract.strip()\n",
    "    articles.append(Article(\n",
    "        id=doc_id,\n",
    "        title=title,\n",
    "        authors=authors,\n",
    "        info=info,\n",
    "        abstract=abstract,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = list()\n",
    "\n",
    "with open('data/cran.qry') as f:\n",
    "    raw_text = f.read()\n",
    "queries_data = raw_text.split('.I')[1:]\n",
    "for (i, query_data) in enumerate(queries_data):\n",
    "    query_data_splitted = query_data.split('.W')\n",
    "    assert len(query_data_splitted) == 2\n",
    "    queries.append(Query(id=i+1, query_string=query_data_splitted[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "\n",
    "Features = namedtuple('Features', ['query_token_count', 'doc_token_count', 'token_df', 'all_docs'])\n",
    "\n",
    "def get_counts_dict(sequence):\n",
    "    result = defaultdict(int)\n",
    "    for el in sequence:\n",
    "        result[el] += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "class Search(object):\n",
    "    def __init__(self, documents, tokenizer, scorer):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._scorer = scorer\n",
    "        self._documents = documents\n",
    "        self._inverted_index = self._build_invert_index(documents)\n",
    "\n",
    "    def _build_invert_index(self, documents):\n",
    "        documents = sorted(documents, key=lambda x: x.id)\n",
    "        index = defaultdict(list)\n",
    "        for doc in documents:\n",
    "            token_counts = get_counts_dict(self._tokenizer(doc.text))\n",
    "            for token, count in token_counts.items():\n",
    "                index[token].append((doc.id, count))\n",
    "        return index\n",
    "\n",
    "    def search(self, query):\n",
    "        query_tokens_counts = get_counts_dict(self._tokenizer(query))\n",
    "        tokens_by_doc = defaultdict(list)\n",
    "        for token, token_count in query_tokens_counts.items():\n",
    "            token_docs = self._inverted_index.get(token, list())\n",
    "            for doc_id, doc_token_count in token_docs:\n",
    "                tokens_by_doc[doc_id].append(Features(\n",
    "                    query_token_count=token_count,\n",
    "                    doc_token_count=doc_token_count,\n",
    "                    token_df=len(token_docs),\n",
    "                    all_docs=len(self._documents),\n",
    "                ))\n",
    "        docs_scores = [(doc_id, self._scorer(doc_info)) for doc_id, doc_info in tokens_by_doc.items()]\n",
    "        return [doc_id for doc_id, doc_score in sorted(docs_scores, key=lambda doc_id_score: doc_id_score[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "STEMMER = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __call__(self, text):\n",
    "        tokens = [el.lower() for el in nltk.word_tokenize(text)]\n",
    "        tokens = [STEMMER.stem(el) for el in tokens if el not in STOPWORDS]\n",
    "        return tokens\n",
    "\n",
    "class Scorer(object):\n",
    "    def __call__(self, doc_data):\n",
    "        return random.randint(1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'player']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tokenizer()(\"Hello, PlAyers a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/v_satanevsky/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = Search(documents=[Document(id=article.id, text=article.abstract) for article in articles], tokenizer=Tokenizer(), scorer=Scorer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.search('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
