{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article = namedtuple('Article', ['id', 'title', 'authors', 'info', 'abstract'])\n",
    "Query = namedtuple('Query', ['id', 'query_string'])\n",
    "Document = namedtuple('Document', ['id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_COUNT = 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = list()\n",
    "with open('data/cran.all.1400') as f:\n",
    "    raw_text = f.read()\n",
    "documents_data = raw_text.split('.I')[1:]\n",
    "assert len(documents_data) == DOCS_COUNT\n",
    "for (i, doc_data) in enumerate(documents_data):\n",
    "    (doc_id, doc_data) = doc_data.split('.T')\n",
    "    doc_id = int(doc_id.strip())\n",
    "    if doc_id != 240:\n",
    "        (title, doc_data) = doc_data.split('.A')\n",
    "    else:\n",
    "        (title, doc_data, _) = doc_data.split('.A')\n",
    "    title = title.strip()\n",
    "    (authors, doc_data) = doc_data.split('.B')\n",
    "    authors = authors.strip()\n",
    "    if doc_id not in (576, 578):\n",
    "        (info, abstract) = doc_data.split('.W')\n",
    "    else:\n",
    "        (info, abstract, _) = doc_data.split('.W')\n",
    "    info = info.strip()\n",
    "    info = abstract.strip()\n",
    "    articles.append(Article(\n",
    "        id=doc_id,\n",
    "        title=title,\n",
    "        authors=authors,\n",
    "        info=info,\n",
    "        abstract=abstract,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = list()\n",
    "\n",
    "with open('data/cran.qry') as f:\n",
    "    raw_text = f.read()\n",
    "queries_data = raw_text.split('.I')[1:]\n",
    "for (i, query_data) in enumerate(queries_data):\n",
    "    query_data_splitted = query_data.split('.W')\n",
    "    assert len(query_data_splitted) == 2\n",
    "    queries.append(Query(id=i+1, query_string=query_data_splitted[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "\n",
    "Features = namedtuple('Features', ['query_token_count', 'doc_token_count', 'token_df', 'all_docs'])\n",
    "\n",
    "def get_counts_dict(sequence):\n",
    "    result = defaultdict(int)\n",
    "    for el in sequence:\n",
    "        result[el] += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "class Search(object):\n",
    "    def __init__(self, documents, tokenizer, scorer):\n",
    "        self._tokenizer = tokenizer\n",
    "        self._scorer = scorer\n",
    "        self._documents = documents\n",
    "        self._inverted_index = self._build_invert_index(documents)\n",
    "\n",
    "    def _build_invert_index(self, documents):\n",
    "        documents = sorted(documents, key=lambda x: x.id)\n",
    "        index = defaultdict(list)\n",
    "        for doc in documents:\n",
    "            token_counts = get_counts_dict(self._tokenizer(doc.text))\n",
    "            for token, count in token_counts.items():\n",
    "                index[token].append((doc.id, count))\n",
    "        return index\n",
    "\n",
    "    def search(self, query):\n",
    "        query_tokens_counts = get_counts_dict(self._tokenizer(query))\n",
    "        tokens_by_doc = defaultdict(list)\n",
    "        for token, token_count in query_tokens_counts.items():\n",
    "            token_docs = self._inverted_index.get(token, list())\n",
    "            for doc_id, doc_token_count in token_docs:\n",
    "                tokens_by_doc[doc_id].append(Features(\n",
    "                    query_token_count=token_count,\n",
    "                    doc_token_count=doc_token_count,\n",
    "                    token_df=len(token_docs),\n",
    "                    all_docs=len(self._documents),\n",
    "                ))\n",
    "        docs_scores = [(doc_id, self._scorer(doc_info)) for doc_id, doc_info in tokens_by_doc.items()]\n",
    "        return [doc_id for doc_id, doc_score in sorted(docs_scores, key=lambda doc_id_score: doc_id_score[1], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "STEMMER = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "class Tokenizer(object):\n",
    "    def __call__(self, text):\n",
    "        tokens = [el.lower() for el in nltk.word_tokenize(text)]\n",
    "        tokens = [STEMMER.stem(el) for el in tokens if el not in STOPWORDS]\n",
    "        return tokens\n",
    "\n",
    "class Scorer(object):\n",
    "    def __call__(self, doc_data):\n",
    "        return random.randint(1, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = Search(documents=[Document(id=article.id, text=article.abstract) for article in articles], tokenizer=Tokenizer(), scorer=Scorer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Evaluator(object):\n",
    "    def __init__(self, answer_pairs_path, possible_queries):\n",
    "        self._possible_queries = possible_queries\n",
    "        self._true_pairs = list()\n",
    "        with open(answer_pairs_path) as f:\n",
    "            for line in f:\n",
    "                qid, did = [int(x) for x in line.split()]\n",
    "                self._true_pairs.append((qid, did))\n",
    "\n",
    "    def _eval_predicted_pairs(self, predicted_pairs):\n",
    "\n",
    "        q2reld = {}\n",
    "        for qid, did in self._true_pairs:\n",
    "            if qid in q2reld.keys():\n",
    "                q2reld[qid].add(did)\n",
    "            else:\n",
    "                q2reld[qid] = set()\n",
    "\n",
    "        q2retrd = {}\n",
    "        for qid, did in predicted_pairs:\n",
    "            qid, did = [int(x) for x in line.split()]\n",
    "            if qid in q2retrd.keys():\n",
    "                q2retrd[qid].append(did)\n",
    "            else:\n",
    "                q2retrd[qid] = []       \n",
    "\n",
    "        N = len(q2retrd.keys())\n",
    "        precision = sum([len(q2reld[q].intersection(q2retrd[q]))*1.0/len(q2retrd[q]) for q in q2retrd.keys()]) / N\n",
    "        recall = sum([len(q2reld[q].intersection(q2retrd[q]))*1.0/len(q2reld[q]) for q in q2retrd.keys()]) / N\n",
    "        print(\"mean precision: {}\\nmean recall: {}\\nmean F-measure: {}\"\\\n",
    "              .format(precision, recall, 2*precision*recall/(precision+recall)))\n",
    "\n",
    "        # MAP@10\n",
    "\n",
    "        MAP = 0.0\n",
    "        for q in q2retrd.keys():\n",
    "            n_results = min(10, len(q2retrd[q]))\n",
    "            avep = np.zeros(n_results)\n",
    "            for i in range(n_results):\n",
    "                avep[i:] += q2retrd[q][i] in q2reld[q]\n",
    "                avep[i] *= (q2retrd[q][i] in q2reld[q]) / (i+1.0)\n",
    "            MAP += sum(avep) / min(n_results, len(q2reld[q]))\n",
    "        print(\"MAP@10: {}\".format(MAP/N))\n",
    "\n",
    "    def __call__(self, engine):\n",
    "        predicted_pairs = list()\n",
    "        for query in self._possible_queries:\n",
    "            found_docs = engine.search(query.query_string)\n",
    "            for doc in found_docs[:10]:\n",
    "                query_result_pairs.append((query.id, doc.id))\n",
    "        return self._eval_predicted_pairs()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
